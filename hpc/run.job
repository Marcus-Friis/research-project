#!/bin/bash

#SBATCH --job-name=llm-test      # Job name
#SBATCH --output=output.out      # Name of output file (%j expands to jobId)
#SBATCH --cpus-per-task=4        # Schedule one core
#SBATCH --time=01:00:00          # Run time (hh:mm:ss) - run for one hour max
#SBATCH --partition=brown        # Run on either the Red or Brown queue
#SBATCH --gres=gpu:4             # If you are using CUDA dependent package
##SBATCH --nodelist=desktop[21,23-24] #Uncommet this for fast AF nodes. SPEEEEEEEEEED. This is the 1.12 baby ðŸš„ðŸ’¨ðŸ’¨ðŸ’¨ðŸ’¨ðŸ’¨ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
echo "Running on $(hostname):"

module load Python/3.9.6-GCCcore-11.2.0
module load CMake/3.21.1-GCCcore-11.2.0
module load CUDA/11.7.0

# run first time
#echo "pip install llama-cpp-python"
#python3 -m pip install --upgrade pip
#CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=2 python3 -m pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir

echo "running python script"
python3 -u main.py
