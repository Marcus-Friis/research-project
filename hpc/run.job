#!/bin/bash

#SBATCH --job-name=llm-test      # Job name
#SBATCH --output=output.out      # Name of output file (%j expands to jobId)
#SBATCH --cpus-per-task=1        # Schedule one core
#SBATCH --time=00:10:00          # Run time (hh:mm:ss) - run for one hour max
#SBATCH --partition=brown        # Run on either the Red or Brown queue
#SBATCH --gres=gpu       	 # If you are using CUDA dependent package

echo "Running on $(hostname):"

# run this every time?
echo "module load Python/3.9.6-GCCcore-11.2.0"
module load Python/3.9.6-GCCcore-11.2.0
echo "module load CMake/3.21.1-GCCcore-11.2.0"
module load CMake/3.21.1-GCCcore-11.2.0
echo "module load CUDA/11.7.0"
module load CUDA/11.7.0
echo "llama-cpp-python"
CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=2 python3 -m pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir

echo "running python script"
python main.py
